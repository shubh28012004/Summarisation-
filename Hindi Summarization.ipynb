{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6aV3jxDAgT8"
      },
      "source": [
        "### Import Libraries\n",
        "This cell imports the necessary libraries, including PyTorch for deep learning and NLTK for tokenizing text. It also sets a random seed to ensure reproducibility of results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_WuxAHxGY_6C",
        "outputId": "3974f196-2503-4ea5-c1d1-5e3fb83313ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
            "[nltk_data]     getaddrinfo failed>\n",
            "[nltk_data] Error loading punkt_tab: <urlopen error [Errno 11001]\n",
            "[nltk_data]     getaddrinfo failed>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import Necessary Libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from rouge import Rouge\n",
        "import os\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57DdPMhgAgT8"
      },
      "source": [
        "### BiLSTM Model for Text Summarization\n",
        "\n",
        "The `BiLSTMSummarizer` class implements a Bidirectional LSTM-based model designed for text summarization. Below is a detailed explanation of its components and the overall forward pass process:\n",
        "\n",
        "1. **Embedding Layer**:\n",
        "    - This layer converts input tokens, represented as indices, into dense vector representations (embeddings) of size `embedding_dim`. It serves as the initial transformation of words into numerical form that can be processed by the model.\n",
        "\n",
        "2. **Bidirectional LSTM Encoder**:\n",
        "    - The encoder consists of a bidirectional LSTM (`nn.LSTM`), which processes the input sequence in both forward and backward directions. This allows the model to capture context from both sides of the input sentence, improving its understanding of long-range dependencies. The LSTM’s hidden size is specified by `hidden_dim`.\n",
        "\n",
        "3. **LSTM Decoder**:\n",
        "    - The decoder is a standard LSTM that receives the concatenated hidden states from both directions of the encoder. It generates the summary sequence one token at a time, using either teacher forcing (feeding in the actual next token) or the previously predicted token.\n",
        "\n",
        "4. **Fully Connected Output Layer**:\n",
        "    - After the decoder processes the sequence, the output is passed through a fully connected layer, which maps the hidden states to a probability distribution over the vocabulary. This distribution is used to predict the next word in the summary.\n",
        "\n",
        "#### Forward Pass Workflow:\n",
        "\n",
        "- **Inputs**:\n",
        "    - `src`: The input article or text sequence.\n",
        "    - `trg`: The target sequence, representing the summary.\n",
        "    - `teacher_forcing_ratio`: A parameter that determines how often teacher forcing is applied. During training, it controls the probability of using the correct next token (from `trg`) instead of the predicted token.\n",
        "\n",
        "- **Steps**:\n",
        "    1. The source sequence (`src`) is embedded into dense vectors using the embedding layer.\n",
        "    2. These embedded vectors are processed by the bidirectional LSTM encoder, which generates hidden states for both forward and backward passes.\n",
        "    3. The hidden states from the two directions are concatenated.\n",
        "    4. The decoder LSTM generates the target sequence (summary) token by token. Depending on the value of `teacher_forcing_ratio`, the decoder either receives the true next token or its own prediction as input at each step.\n",
        "\n",
        "- **Output**:\n",
        "    - The model outputs a sequence of predicted tokens representing the generated summary, with each token being the most likely word based on the decoder’s output probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mCIhYCOdY_6E"
      },
      "outputs": [],
      "source": [
        "# Define the BiLSTM model for text summarization\n",
        "class BiLSTMSummarizer(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(BiLSTMSummarizer, self).__init__()\n",
        "        # Embedding layer to convert input words to word embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM encoder with bidirectionality to capture context from both directions\n",
        "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "\n",
        "        # Decoder with LSTM, input is the output from the encoder (concatenated hidden states)\n",
        "        self.decoder = nn.LSTM(embedding_dim, hidden_dim * 2, batch_first=True)\n",
        "\n",
        "        # Fully connected layer to map the hidden states to the vocabulary size (output)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    # Forward pass through the model\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[0]  # Get batch size\n",
        "        trg_len = trg.shape[1]  # Get the length of the target sequence\n",
        "        trg_vocab_size = self.fc.out_features  # Get the output vocabulary size\n",
        "\n",
        "        # Initialize the output tensor with zeros (batch_size, trg_len, vocab_size)\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(src.device)\n",
        "\n",
        "        # Pass the source sentence through the embedding layer\n",
        "        embedded = self.embedding(src)\n",
        "\n",
        "        # Pass the embeddings through the bidirectional LSTM encoder\n",
        "        enc_output, (hidden, cell) = self.encoder(embedded)\n",
        "\n",
        "        # Combine the hidden states from both directions (concatenate)\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1).unsqueeze(0)\n",
        "        cell = torch.cat((cell[-2,:,:], cell[-1,:,:]), dim=1).unsqueeze(0)\n",
        "\n",
        "        # Start decoding with the first token (usually <sos>)\n",
        "        input = trg[:, 0]\n",
        "\n",
        "        # Loop over each time step in the target sequence\n",
        "        for t in range(1, trg_len):\n",
        "            input_embedded = self.embedding(input).unsqueeze(1)  # Embed the current input token\n",
        "            output, (hidden, cell) = self.decoder(input_embedded, (hidden, cell))  # Decode one step\n",
        "            prediction = self.fc(output.squeeze(1))  # Pass decoder output through fully connected layer\n",
        "\n",
        "            outputs[:, t] = prediction  # Store the prediction at current time step\n",
        "\n",
        "            # Use teacher forcing (feeding correct output token back into the model)\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            top1 = prediction.argmax(1)  # Get the predicted token\n",
        "            input = trg[:, t] if teacher_force else top1  # Decide whether to use teacher forcing or not\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCuzLFlBAgT9"
      },
      "source": [
        "### Dataset Class for Text Summarization\n",
        "\n",
        "The `SummarizationDataset` class is a custom PyTorch `Dataset` that prepares data (articles and their corresponding summaries) for the text summarization task. Here's a breakdown of its functionality:\n",
        "\n",
        "1. **Initialization (`__init__`)**:\n",
        "    - **Parameters**:\n",
        "        - `articles`: A list containing the source texts (input articles).\n",
        "        - `summaries`: A list containing the target texts (summaries).\n",
        "        - `vocab`: A dictionary mapping words to their corresponding indices (vocabulary).\n",
        "        - `max_length`: The maximum length for input and output sequences, used for padding or truncating.\n",
        "\n",
        "    - **Purpose**:\n",
        "        - It initializes the dataset with articles, summaries, and the vocabulary while setting a maximum sequence length for consistent data processing.\n",
        "\n",
        "2. **Dataset Length (`__len__`)**:\n",
        "    - **Purpose**:\n",
        "        - This method returns the number of samples in the dataset by returning the length of the `articles` list.\n",
        "\n",
        "3. **Fetching Data Sample (`__getitem__`)**:\n",
        "    - **Purpose**:\n",
        "        - This method retrieves one sample (a pair of an article and its summary) from the dataset, converts the text into a sequence of token indices, and ensures the sequence length is consistent by padding or truncating the sequences.\n",
        "    \n",
        "    - **Process**:\n",
        "        - Each article and summary is tokenized by converting the text into a list of indices based on the provided vocabulary (`vocab`).\n",
        "        - Special tokens like `<sos>` (start of sentence) and `<eos>` (end of sentence) are added at the beginning and end of each sequence.\n",
        "        - If any words are not found in the vocabulary, they are replaced with the `<unk>` (unknown) token.\n",
        "        - Padding (`<pad>`) is added to sequences that are shorter than the maximum length (`max_length`), ensuring all sequences are of the same length for batch processing.\n",
        "\n",
        "    - **Output**:\n",
        "        - The method returns two tensors: one for the article and one for the corresponding summary, both padded or truncated to the same length (`max_length`).\n",
        "\n",
        "#### Special Tokens:\n",
        "- **`<sos>`**: Marks the start of a sentence.\n",
        "- **`<eos>`**: Marks the end of a sentence.\n",
        "- **`<pad>`**: Used to pad sequences to ensure consistent length across the dataset.\n",
        "- **`<unk>`**: Represents unknown or out-of-vocabulary words.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gBEvTN3KY_6F"
      },
      "outputs": [],
      "source": [
        "# Dataset class to prepare data for summarization\n",
        "class SummarizationDataset(Dataset):\n",
        "    def __init__(self, articles, summaries, vocab, max_length=100):\n",
        "        self.articles = articles  # List of articles (source text)\n",
        "        self.summaries = summaries  # List of summaries (target text)\n",
        "        self.vocab = vocab  # Vocabulary mapping\n",
        "        self.max_length = max_length  # Maximum sequence length for padding/truncating\n",
        "\n",
        "    # Return the length of the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.articles)\n",
        "\n",
        "    # Return a sample of data (article, summary) as tensors\n",
        "    def __getitem__(self, idx):\n",
        "        article = self.articles[idx]\n",
        "        summary = self.summaries[idx]\n",
        "\n",
        "        # Convert article to a list of token indices\n",
        "        article_indices = [self.vocab['<sos>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in article][:self.max_length-2] + [self.vocab['<eos>']]\n",
        "        summary_indices = [self.vocab['<sos>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in summary][:self.max_length-2] + [self.vocab['<eos>']]\n",
        "\n",
        "        # Pad sequences to max_length\n",
        "        article_indices = article_indices + [self.vocab['<pad>']] * (self.max_length - len(article_indices))\n",
        "        summary_indices = summary_indices + [self.vocab['<pad>']] * (self.max_length - len(summary_indices))\n",
        "\n",
        "        return torch.tensor(article_indices), torch.tensor(summary_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZtwymucAgT-"
      },
      "source": [
        "### Data Loading and Preprocessing\n",
        "\n",
        "1. **Load Data (`load_data`)**:\n",
        "    - Loads headlines and contents from a CSV file.\n",
        "    - **Input**: CSV file path.\n",
        "    - **Output**: Lists of headlines and content.\n",
        "    \n",
        "2. **Tokenize Text (`tokenize`)**:\n",
        "    - Tokenizes and lowercases the text using NLTK.\n",
        "    - **Input**: Text string.\n",
        "    - **Output**: Tokenized words.\n",
        "\n",
        "3. **Build Vocabulary (`build_vocab`)**:\n",
        "    - Creates a word-to-index vocabulary based on word frequency.\n",
        "    - **Input**: Tokenized texts, minimum frequency (`min_freq=2`).\n",
        "    - **Output**: `word2idx` and `idx2word` mappings, including special tokens (`<pad>`, `<unk>`, `<sos>`, `<eos>`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tgn_dCLbY_6H"
      },
      "outputs": [],
      "source": [
        "# Load the dataset from a CSV file\n",
        "file_path = r\"D:\\hindi_news_dataset.csv\"\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df['Headline'].tolist(), df['Content'].tolist() \n",
        "\n",
        "# Tokenize text using word_tokenize from nltk\n",
        "def tokenize(text):\n",
        "    return word_tokenize(text.lower())  # Tokenize and lowercase the text\n",
        "\n",
        "# Build vocabulary from the dataset\n",
        "def build_vocab(texts, min_freq=2):\n",
        "    word_freq = Counter()  # Count word frequencies\n",
        "    for text in texts:\n",
        "        word_freq.update(text)\n",
        "\n",
        "    # Initialize special tokens\n",
        "    vocab = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}\n",
        "\n",
        "    # Add words with frequency >= min_freq\n",
        "    for word, freq in word_freq.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[word] = len(vocab)\n",
        "\n",
        "    return vocab, {v: k for k, v in vocab.items()}  # Return word2idx and idx2word mappings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8194OwXAgT-"
      },
      "source": [
        "### Data Preparation for Text Summarization\n",
        "\n",
        "1. **Load and Tokenize Data**:\n",
        "    - The articles and summaries are loaded from the CSV file and tokenized into word tokens.\n",
        "    - **Steps**:\n",
        "        - `articles, summaries = load_data(file_path)` loads the data.\n",
        "        - `tokenized_articles = [tokenize(article) for article in articles]` tokenizes each article.\n",
        "        - `tokenized_summaries = [tokenize(summary) for summary in summaries]` tokenizes each summary.\n",
        "\n",
        "2. **Build Vocabulary**:\n",
        "    - The vocabulary is built from the tokenized articles and summaries.\n",
        "    - **Step**:\n",
        "        - `vocab, inv_vocab = build_vocab(tokenized_articles + tokenized_summaries)` creates word-to-index and index-to-word mappings.\n",
        "\n",
        "3. **Split Data**:\n",
        "    - The tokenized data is split into training, validation, and test sets.\n",
        "    - **Steps**:\n",
        "        - `train_test_split` is used to split the data, first into training and test sets (80-20 split), then further splitting the training set to create a validation set (10% of training).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lHfREZHwY_6I"
      },
      "outputs": [],
      "source": [
        "# Load and tokenize the articles and summaries\n",
        "articles, summaries = load_data(file_path)\n",
        "tokenized_articles = [tokenize(article) for article in articles]\n",
        "tokenized_summaries = [tokenize(summary) for summary in summaries]\n",
        "\n",
        "# Build vocabulary\n",
        "vocab, inv_vocab = build_vocab(tokenized_articles + tokenized_summaries)\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "train_articles, test_articles, train_summaries, test_summaries = train_test_split(tokenized_articles, tokenized_summaries, test_size=0.2, random_state=42)\n",
        "train_articles, val_articles, train_summaries, val_summaries = train_test_split(train_articles, train_summaries, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CP18TCZfY_6J"
      },
      "outputs": [],
      "source": [
        "# Create datasets using the tokenized data and vocab\n",
        "train_dataset = SummarizationDataset(train_articles, train_summaries, vocab, max_length=50)\n",
        "val_dataset = SummarizationDataset(val_articles, val_summaries, vocab, max_length=50)\n",
        "test_dataset = SummarizationDataset(test_articles, test_summaries, vocab, max_length=50)\n",
        "\n",
        "# Create data loaders to feed data in batches\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-X7pLGHsY_6J"
      },
      "outputs": [],
      "source": [
        "# Initialize model and hyperparameters\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # Use GPU if available\n",
        "\n",
        "vocab_size = len(vocab)   # Size of the vocabulary\n",
        "embedding_dim = 300       # Size of word embeddings\n",
        "hidden_dim = 512          # Size of LSTM hidden state\n",
        "output_dim = vocab_size   # Output size, generally the size of the vocabulary\n",
        "\n",
        "# Initialize the BiLSTM model and move it to the device (GPU/CPU)\n",
        "model = BiLSTMSummarizer(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT2l2LK0AgT_"
      },
      "source": [
        "### Training Function\n",
        "\n",
        "The `train` function performs the training process, updating model weights using batches of data.\n",
        "\n",
        "1. **Training Mode**:\n",
        "    - `model.train()` enables training-specific behaviors (like dropout).\n",
        "\n",
        "2. **Batch Processing**:\n",
        "    - Each batch (`src`, `trg`) is passed through the model, and outputs are reshaped for loss calculation.\n",
        "\n",
        "3. **Loss and Backpropagation**:\n",
        "    - Loss is calculated between predicted and target sequences, followed by backpropagation.\n",
        "\n",
        "4. **Gradient Clipping**:\n",
        "    - Gradients are clipped to prevent explosion (`clip=1`).\n",
        "\n",
        "5. **Optimizer Step**:\n",
        "    - Weights are updated using the optimizer.\n",
        "\n",
        "6. **Return**:\n",
        "    - Average loss per epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UZGJGt5-Y_6K"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train(model, iterator, optimizer, criterion, device, clip=1, teacher_forcing_ratio=0.5):\n",
        "    model.train()  # Set model to training mode\n",
        "    epoch_loss = 0\n",
        "    for batch in tqdm(iterator, desc=\"Training\"):  # Iterate over batches\n",
        "        src, trg = batch\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Clear gradients\n",
        "        output = model(src, trg, teacher_forcing_ratio)  # Forward pass\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:].reshape(-1, output_dim)  # Reshape output for loss calculation\n",
        "        trg = trg[:, 1:].reshape(-1)  # Flatten target sequence\n",
        "\n",
        "        loss = criterion(output, trg)  # Calculate loss\n",
        "        loss.backward()  # Backpropagate\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  # Clip gradients to avoid exploding gradient\n",
        "\n",
        "        optimizer.step()  # Update parameters\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkVaqJpgAgT_"
      },
      "source": [
        "### Evaluation Function\n",
        "\n",
        "The `evaluate` function assesses the model's performance on validation or test data, without updating the model's weights.\n",
        "\n",
        "1. **Evaluation Mode**:\n",
        "    - `model.eval()` disables training-specific behaviors like dropout.\n",
        "  \n",
        "2. **No Gradient Calculation**:\n",
        "    - `torch.no_grad()` ensures no gradients are computed, saving memory and speeding up the evaluation.\n",
        "\n",
        "3. **Batch Processing**:\n",
        "    - For each batch, the model is run with teacher forcing disabled (`teacher_forcing_ratio=0`).\n",
        "    - The output and target are reshaped to match dimensions for loss calculation.\n",
        "\n",
        "4. **Loss Calculation**:\n",
        "    - Loss is computed between model predictions and target sequences.\n",
        "\n",
        "5. **Return**:\n",
        "    - The average loss over all batches is returned.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sUtmmZLIY_6L"
      },
      "outputs": [],
      "source": [
        "# Evaluation function\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for batch in tqdm(iterator, desc=\"Evaluating\"):\n",
        "            src, trg = batch\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            output = model(src, trg, 0)  # Turn off teacher forcing during evaluation\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:].reshape(-1, output_dim)  # Reshape output for loss calculation\n",
        "            trg = trg[:, 1:].reshape(-1)  # Flatten target sequence\n",
        "\n",
        "            loss = criterion(output, trg)  # Calculate loss\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH6H2fzGAgT_"
      },
      "source": [
        "### Beam Search for Text Generation\n",
        "\n",
        "The `beam_search` function implements the beam search algorithm for sequence generation. This approach is used to generate the best possible output sequence by exploring multiple hypotheses at each decoding step.\n",
        "\n",
        "1. **Embedding and Encoding**:\n",
        "    - The input sequence is first embedded using the model's embedding layer.\n",
        "    - The embedded sequence is passed through the encoder (a bi-directional LSTM), and the final hidden and cell states are obtained.\n",
        "\n",
        "2. **Beam Initialization**:\n",
        "    - The beam is initialized with the start-of-sequence token (`<sos>`), a score of 0, and the hidden and cell states from the encoder.\n",
        "\n",
        "3. **Beam Search Process**:\n",
        "    - For each time step, each sequence in the current beam is extended by predicting the next token using the decoder.\n",
        "    - The top `beam_width` predictions (tokens with the highest probabilities) are selected.\n",
        "    - These new sequences are added to the beam, and the beam is updated with the top `beam_width` sequences based on their cumulative scores.\n",
        "\n",
        "4. **Handling Sequence Completion**:\n",
        "    - If a sequence reaches the end-of-sequence token (`<eos>`) and is of sufficient length, it is added to the list of complete hypotheses.\n",
        "\n",
        "5. **Final Sequence Selection**:\n",
        "    - Once the beam search is complete or the maximum length is reached, the best sequence is selected from the completed hypotheses.\n",
        "    - If no sequence ends with `<eos>`, the best incomplete sequence is chosen.\n",
        "\n",
        "6. **Output**:\n",
        "    - The selected sequence of token indices is converted back to words using the `inv_vocab` (index-to-word) mapping.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dFZaxsetY_6L"
      },
      "outputs": [],
      "source": [
        "def beam_search(model, src, vocab, inv_vocab, beam_width=3, max_length=50, min_length=10, device='gpu'):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Embedding the input sequence\n",
        "        embedded = model.embedding(src)  # shape: (batch_size, seq_len, embedding_dim)\n",
        "        enc_output, (hidden, cell) = model.encoder(embedded)  # LSTM encoder output\n",
        "\n",
        "        # In case of bi-directional LSTM, combine the hidden states\n",
        "        if model.encoder.bidirectional:\n",
        "            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)  # shape: (batch_size, hidden_dim)\n",
        "            cell = torch.cat((cell[-2, :, :], cell[-1, :, :]), dim=1)        # shape: (batch_size, hidden_dim)\n",
        "        else:\n",
        "            hidden = hidden[-1, :, :]  # Take the last layer if not bi-directional\n",
        "            cell = cell[-1, :, :]      # Take the last layer if not bi-directional\n",
        "\n",
        "        # Now we process one sequence at a time, so set batch size to 1\n",
        "        hidden = hidden.unsqueeze(0)  # shape: (1, batch_size, hidden_dim)\n",
        "        cell = cell.unsqueeze(0)      # shape: (1, batch_size, hidden_dim)\n",
        "\n",
        "        # Initialize the beam with the start-of-sequence token\n",
        "        beam = [([vocab['<sos>']], 0, hidden[:, 0:1, :], cell[:, 0:1, :])]  # Start with one sequence\n",
        "        complete_hypotheses = []\n",
        "\n",
        "        # Perform beam search\n",
        "        for t in range(max_length):\n",
        "            new_beam = []\n",
        "            for seq, score, hidden, cell in beam:\n",
        "                # If end-of-sequence token is reached and length is >= min_length, add to complete hypotheses\n",
        "                if seq[-1] == vocab['<eos>'] and len(seq) >= min_length:\n",
        "                    complete_hypotheses.append((seq, score))\n",
        "                    continue\n",
        "\n",
        "                # Prepare the input for the decoder (last predicted token)\n",
        "                input = torch.LongTensor([seq[-1]]).unsqueeze(0).to(device)  # shape: (1, 1)\n",
        "                input_embedded = model.embedding(input)  # shape: (1, 1, embedding_dim)\n",
        "\n",
        "                # Pass through the decoder with the current hidden and cell states\n",
        "                output, (hidden, cell) = model.decoder(input_embedded, (hidden, cell))  # hidden, cell are (1, 1, hidden_dim)\n",
        "                predictions = model.fc(output.squeeze(1))  # shape: (1, vocab_size)\n",
        "\n",
        "                # Prevent EOS if sequence is shorter than minimum length\n",
        "                if len(seq) < min_length:\n",
        "                    predictions[0][vocab['<eos>']] = float('-inf')\n",
        "\n",
        "                # Get top beam_width predictions\n",
        "                top_preds = torch.topk(predictions, beam_width, dim=1)\n",
        "\n",
        "                # For each top prediction, extend the sequence and update the beam\n",
        "                for i in range(beam_width):\n",
        "                    new_seq = seq + [top_preds.indices[0][i].item()]\n",
        "                    new_score = score - top_preds.values[0][i].item()  # Negative log probability\n",
        "                    new_hidden = hidden.clone()\n",
        "                    new_cell = cell.clone()\n",
        "                    new_beam.append((new_seq, new_score, new_hidden, new_cell))\n",
        "\n",
        "            # Sort by score and keep top beam_width sequences\n",
        "            beam = sorted(new_beam, key=lambda x: x[1])[:beam_width]\n",
        "\n",
        "            if len(complete_hypotheses) >= beam_width:\n",
        "                break\n",
        "\n",
        "        # Sort and return the best sequence\n",
        "        complete_hypotheses = sorted(complete_hypotheses, key=lambda x: x[1])\n",
        "        if complete_hypotheses:\n",
        "            best_seq = complete_hypotheses[0][0]\n",
        "        else:\n",
        "            best_seq = beam[0][0]\n",
        "\n",
        "    # Convert sequence of indices back to words\n",
        "    return [inv_vocab[idx] for idx in best_seq if idx not in [vocab['<sos>'], vocab['<eos>'], vocab['<pad>']]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iYMa8KYbY_6M"
      },
      "outputs": [],
      "source": [
        "# Save model function\n",
        "def save_model(model, vocab, filepath):\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'vocab': vocab\n",
        "    }, filepath)\n",
        "    print(f\"Model saved to {'Copy_of_Hindi_Summarization_Beam_Search copy.ipynb'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fCI_lkWhY_6M"
      },
      "outputs": [],
      "source": [
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WyyssAHKY_6M",
        "outputId": "0b91ef58-6819-4a38-d15d-bceaade57b1f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [48:28<00:00,  1.39s/it] \n",
            "Evaluating: 100%|██████████| 232/232 [00:56<00:00,  4.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 5.205\n",
            "\t Val. Loss: 6.031\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [36:46<00:00,  1.06s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [00:56<00:00,  4.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 02\n",
            "\tTrain Loss: 3.184\n",
            "\t Val. Loss: 4.814\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [36:41<00:00,  1.06s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [00:56<00:00,  4.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 03\n",
            "\tTrain Loss: 2.267\n",
            "\t Val. Loss: 4.081\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [36:30<00:00,  1.05s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [00:55<00:00,  4.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 04\n",
            "\tTrain Loss: 1.783\n",
            "\t Val. Loss: 3.636\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [36:35<00:00,  1.05s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [00:56<00:00,  4.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 05\n",
            "\tTrain Loss: 1.458\n",
            "\t Val. Loss: 3.278\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [36:46<00:00,  1.06s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [00:56<00:00,  4.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 06\n",
            "\tTrain Loss: 1.235\n",
            "\t Val. Loss: 3.069\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [36:43<00:00,  1.06s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [00:56<00:00,  4.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 07\n",
            "\tTrain Loss: 1.076\n",
            "\t Val. Loss: 2.922\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [36:45<00:00,  1.06s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [00:55<00:00,  4.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 08\n",
            "\tTrain Loss: 0.957\n",
            "\t Val. Loss: 2.742\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [36:36<00:00,  1.05s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [00:55<00:00,  4.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 09\n",
            "\tTrain Loss: 0.859\n",
            "\t Val. Loss: 2.773\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2087/2087 [36:37<00:00,  1.05s/it]\n",
            "Evaluating: 100%|██████████| 232/232 [00:56<00:00,  4.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10\n",
            "\tTrain Loss: 0.780\n",
            "\t Val. Loss: 2.556\n",
            "Model saved to Copy_of_Hindi_Summarization_Beam_Search copy.ipynb\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\t Val. Loss: {val_loss:.3f}')\n",
        "\n",
        "    # Save model if validation loss improves\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        save_model(model, vocab, 'best_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3Di3pobeY_6N"
      },
      "outputs": [],
      "source": [
        "# Load model function\n",
        "def load_model(filepath, device):\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "    vocab = checkpoint['vocab']\n",
        "    model = BiLSTMSummarizer(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    return model, checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DPyf-nlSY_6N",
        "outputId": "4bb48d10-785b-4019-c8b5-dddc650e6a43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_19060\\114305855.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filepath, map_location=device)\n",
            "Evaluating: 100%|██████████| 580/580 [02:20<00:00,  4.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 2.539\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating summaries: 100%|██████████| 580/580 [02:25<00:00,  3.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE scores:\n",
            "{'rouge-1': {'r': 0.8053488385281857, 'p': 0.8311310077492784, 'f': 0.8165485464083134}, 'rouge-2': {'r': 0.7288117552363507, 'p': 0.7373940617458541, 'f': 0.7327340576196352}, 'rouge-l': {'r': 0.7854555646572221, 'p': 0.8081892196140378, 'f': 0.795352900614367}}\n"
          ]
        }
      ],
      "source": [
        "# Load the best model for testing\n",
        "best_model, _ = load_model('best_model.pth', device)\n",
        "\n",
        "# Test the model\n",
        "test_loss = evaluate(best_model, test_loader, criterion, device)\n",
        "print(f'Test Loss: {test_loss:.3f}')\n",
        "\n",
        "# Evaluate using ROUGE score\n",
        "rouge = Rouge()\n",
        "best_model.eval()\n",
        "predictions = []\n",
        "references = []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Generating summaries\"):\n",
        "        src, trg = batch\n",
        "        src = src.to(device)\n",
        "        pred = beam_search(best_model, src, vocab, inv_vocab, min_length=10, device=device)  # Set minimum length\n",
        "        predictions.extend([' '.join(pred)])\n",
        "        references.extend([' '.join([inv_vocab[idx.item()] for idx in trg[0] if idx.item() not in [vocab['<sos>'], vocab['<eos>'], vocab['<pad>']]])])\n",
        "\n",
        "# Ensure all predictions meet the minimum length\n",
        "min_length = 10  # Set this to your desired minimum length\n",
        "predictions = [p if len(p.split()) >= min_length else p + ' ' + ' '.join(['<pad>'] * (min_length - len(p.split()))) for p in predictions]\n",
        "\n",
        "scores = rouge.get_scores(predictions, references, avg=True)\n",
        "print(\"ROUGE scores:\")\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Y68PKEeYY_6N",
        "outputId": "92d0777f-cd84-4b63-efb5-8a0e8a4b2447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading pre-trained model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_19060\\114305855.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(filepath, map_location=device)\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading pre-trained model...\")\n",
        "trained_model, checkpoint = load_model('best_model.pth', device)\n",
        "vocab = checkpoint['vocab']\n",
        "inv_vocab = {v: k for k, v in vocab.items()}\n",
        "trained_model = trained_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CMv-l-VmY_6O"
      },
      "outputs": [],
      "source": [
        "# Modified Summarization bot\n",
        "def summarize_text(model, vocab, inv_vocab, text, max_length=100, min_length=10, beam_width=3, device='cpu', debug=False):\n",
        "    model.eval()\n",
        "    tokens = tokenize(text)[:max_length]\n",
        "    indices = [vocab['<sos>']] + [vocab.get(token, vocab['<unk>']) for token in tokens] + [vocab['<eos>']]\n",
        "    src = torch.LongTensor(indices).unsqueeze(0).to(device)\n",
        "\n",
        "    summary = beam_search(model, src, vocab, inv_vocab, beam_width, max_length, min_length, device)\n",
        "\n",
        "    if debug:\n",
        "        print(\"Input tokens:\", tokens)\n",
        "        print(\"Input indices:\", indices)\n",
        "        print(\"Generated indices:\", [vocab[word] for word in summary])\n",
        "        print(\"Summary length:\", len(summary))\n",
        "\n",
        "    return ' '.join(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-6HLvvrkY_6O",
        "outputId": "b4210c5f-ccef-496b-9339-27f7b8ec863a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input tokens: ['ऑस्ट्रेलिया', 'ने', 'ब्लूमफोनटीन', 'में', 'पहले', 'वनडे', 'में', 'दक्षिण', 'अफ्रीका', 'को', '3-विकेट', 'से', 'हरा', 'दिया।', 'यह', '12', 'वर्षों', 'में', 'दक्षिण', 'अफ्रीका', 'के', 'खिलाफ', 'उसकी', 'धरती', 'पर', 'ऑस्ट्रेलिया', 'की', 'पहली', 'वनडे', 'जीत', 'है।', 'ऑस्ट्रेलिया', 'का', 'स्कोर', '16.3', 'ओवर', 'में', '113/7', 'था', 'लेकिन', 'मार्नस', 'लबुशेन', 'और', 'ऐश्टन', 'एगर', 'की', '112', '*', 'रनों', 'की', 'साझेदारी', 'की', 'बदौलत', 'उसने', '40.2', 'ओवर', 'में', 'लक्ष्य', 'हासिल', 'कर', 'लिया।']\n",
            "Input indices: [2, 3351, 83, 29389, 10, 1276, 3352, 10, 3184, 965, 76, 29390, 37, 3192, 27649, 229, 605, 489, 10, 3184, 965, 12, 323, 431, 3771, 98, 3351, 8, 575, 3352, 2706, 27646, 3351, 24, 3490, 29391, 3396, 10, 29392, 28, 2458, 3769, 3770, 73, 29393, 29394, 8, 10147, 3628, 8210, 8, 11848, 8, 3884, 4727, 29395, 3396, 10, 1983, 3806, 103, 27891, 3]\n",
            "Generated indices: [86, 12, 1648, 56, 3352, 490, 3358, 1958, 12, 1276, 1243, 10, 3351, 12, 323, 405, 679, 12, 2486, 5367, 3405, 162, 97, 12, 30, 3443, 3382, 212, 2390, 212, 98, 1206, 103, 1524, 41, 56, 3351, 83, 1234, 1110, 3822, 1373, 12, 3362, 1243, 10, 7171, 10, 508, 14737, 6361, 508, 14737, 12, 323, 1243, 10, 229, 1238, 1243, 27646]\n",
            "Summary length: 61\n",
            "Generated Summary:\n",
            "रिपोर्ट्स के मुताबिक , वनडे विश्व कप 2023 के पहले मैच में ऑस्ट्रेलिया के खिलाफ अपनी टीम के शुरुआती 2,000 रन पूरे होने के बाद ड्रेसिंग रूम ' x ' पर शेयर कर लिखा है , ऑस्ट्रेलिया ने इस बार आईपीएल 2024 के फाइनल मैच में अहमदाबाद में गुजरात टाइटंस ( गुजरात टाइटंस के खिलाफ मैच में यह पहला मैच है।\n",
            "Summary length: 61\n"
          ]
        }
      ],
      "source": [
        "# Example usage of the summarization bot\n",
        "input_text = \"ऑस्ट्रेलिया ने ब्लूमफोनटीन में पहले वनडे में दक्षिण अफ्रीका को 3-विकेट से हरा दिया। यह 12 वर्षों में दक्षिण अफ्रीका के खिलाफ उसकी धरती पर ऑस्ट्रेलिया की पहली वनडे जीत है। ऑस्ट्रेलिया का स्कोर 16.3 ओवर में 113/7 था लेकिन मार्नस लबुशेन और ऐश्टन एगर की 112* रनों की साझेदारी की बदौलत उसने 40.2 ओवर में लक्ष्य हासिल कर लिया।\"\n",
        "summary = summarize_text(trained_model, vocab, inv_vocab, input_text, min_length=10, device=device, debug=True)\n",
        "print(\"Generated Summary:\")\n",
        "print(summary)\n",
        "print(\"Summary length:\", len(summary.split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1MObqUOAgUB"
      },
      "source": [
        "**ROUGE Score Evaluation**\n",
        "\n",
        "The following ROUGE scores evaluate the performance of the text summarization model in terms of ROUGE-1, ROUGE-2, and ROUGE-L.\n",
        "\n",
        "**ROUGE-1 (Unigrams)**\n",
        "\n",
        "Recall: 0.805\n",
        "\n",
        "Precision: 0.831\n",
        "\n",
        "F1-score: 0.817\n",
        "\n",
        "Analysis:\n",
        "\n",
        "ROUGE-1 measures the overlap of unigrams (individual words) between the generated summaries and reference summaries. The recall score of 0.805 means that the model captures 80.5% of relevant unigrams from the reference summaries. The precision score of 0.831 shows that 83.1% of the unigrams generated by the model are correct. The F1-score, which balances precision and recall, is 0.817, indicating strong overall performance in capturing individual words.\n",
        "\n",
        "**ROUGE-2 (Bigrams)**\n",
        "\n",
        "Recall: 0.729\n",
        "\n",
        "Precision: 0.737\n",
        "\n",
        "F1-score: 0.733\n",
        "\n",
        "Analysis:\n",
        "ROUGE-2 focuses on the overlap of bigrams (pairs of consecutive words). The recall of 0.729 indicates that 72.9% of the relevant bigrams from the reference summaries are captured by the model. The precision score of 0.737 means that 73.7% of the bigrams in the generated summaries are correct. The F1-score of 0.733 reflects the model's reasonable performance in capturing longer word sequences, though it is lower than ROUGE-1, as expected, due to the increased complexity of matching bigrams.\n",
        "\n",
        "**ROUGE-L (Longest Common Subsequence)**\n",
        "\n",
        "Recall: 0.785\n",
        "\n",
        "Precision: 0.808\n",
        "\n",
        "F1-score: 0.795\n",
        "\n",
        "Analysis:\n",
        "ROUGE-L evaluates the longest common subsequence between the generated and reference summaries, focusing on capturing the overall structure of the text. The recall score of 0.785 shows that the model aligns well with the reference summaries, capturing 78.5% of the longest subsequences. Precision is higher at 0.808, meaning that 80.8% of the generated subsequences are correct. The F1-score of 0.795 demonstrates strong performance in maintaining the structural integrity of the summaries, comparable to ROUGE-1.\n",
        "\n",
        "**Summary of Comparison:**\n",
        "\n",
        "ROUGE-1 has the highest scores, reflecting the model's strength in capturing individual words accurately.\n",
        "\n",
        "ROUGE-2 shows slightly lower scores, indicating that the model finds it more challenging to capture exact bigram (two-word sequence) matches.\n",
        "\n",
        "ROUGE-L closely follows ROUGE-1 in performance, highlighting the model’s ability to capture the overall sequence structure and flow of the summaries.\n",
        "\n",
        "Overall, the model performs well in generating summaries with strong word overlap (ROUGE-1) and sequence structure (ROUGE-L), though it shows some difficulty in matching consecutive word pairs (ROUGE-2)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
